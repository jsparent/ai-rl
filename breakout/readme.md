| Algorithmes d&#39;apprentissage par renforcement |


# Travail de session
27 novembre - Automne 2020 
Jean-S√©bastien Parent

![Breakout](images/breakout.png)


# Mise en contexte

Dans le cadre du cours, nous devons explorer un cas concret d&#39;application d&#39;un syst√®me d&#39;apprentissage par renforcement. J&#39;ai choisi d&#39;utiliser le jeu ¬´ breakout ¬ª, un classique d&#39;Atari, pour relever le d√©fi et essayer de rendre autonome un mod√®le afin de maximiser le score obtenu dans une partie; j&#39;avoue avoir fait ce choix car j&#39;ai toujours aim√© le principe du jeu, simple en apparence, mais qui demande quelques strat√©gies et ne pardonne pas le manque de r√©flexes.

Heureusement, pour nous aider √† mettre en place un environnement, la librairie OpenAI Gym permet de facilement mettre en place des environnements pr√© faits qui permettent d&#39;interagir avec le jeu et de simuler diff√©rentes actions.

De mani√®re simplifi√©e, on peut voir ces environnements comme des bo√Ætes noires o√π chaque instant un agent (notre mod√®le) peut intervenir afin de poser une action, recevoir un signal de r√©compense et de fin de jeu ainsi que l&#39;√©tat du jeu.

- √âtat initial
- Action pos√©e par l&#39;Agent
- R√©compense + nouvel √©tat
- Action pos√©e par l&#39;Agent
- R√©compense + nouvel √©tat
- etc.

Les diff√©rents environnements d&#39;Atari offert par la librairie OpenAI Gym viennent parfois en versions distinctes; par exemple, dans le cas de ¬´ Breakout ¬ª, il existe la version RAM et la version √âcran :

- La version RAM repr√©sente l&#39;√©tat √† tout instant du jeu sous forme de 128 bytes (valeurs de 0 √† 255) repr√©sentant toutes les informations du jeu √† cet instant.
- La version √âcran repr√©sente les pixels de l&#39;√©cran sous forme de matrice de pixels (largeur, hauteur) et o√π chaque pixel peut avoir jusqu&#39;√† 128 couleurs.

Dans mon cas, j&#39;ai choisi d&#39;utiliser la version RAM pour isoler le processus de traitement d&#39;images de l&#39;apprentissage par renforcement en lui-m√™me, on se concentre donc sur la m√©moire brute directement dans ce cas.

Le concept du jeu est fort simple; des rang√©es de brique en haut de l&#39;√©cran doivent √™tre d√©truites en utilisant une balle libre et une palette que le joueur contr√¥le.

- Lorsque la balle touche la palette, un mur ou une brique, elle rebondit dans la direction oppos√©e (X et Y)
- Si la balle tombe dans l&#39;espace sous la palette, le joueur perd une vie
- La partie compte 5 vies au total et le score est cumulatif sur ces 5 vies

Un √©cran de jeu Breakout ressemble √† ceci :

![Breakout](images/breakout.png)

Il est possible, pour l&#39;agent, de faire quatre (4) actions distinctes sur cette version du jeu (Breakout-ram-v0) :

- NOOP (aucune action)
- FIRE (lancement de la balle)
- LEFT (d√©placement √† gauche)
- RIGHT (d√©placement √† droite)

L&#39;environnement, suite √† une action, retourne une r√©compense si une brique a √©t√© d√©truite (1) ou rien (0) dans les autres cas.

- Si le joueur n&#39;a plus de vie, il renvoie un signal de fin de partie √©galement (done)

# Processus de d√©cision de Markov

Un processus de d√©cision markovien (PDM) est un environnement dans lequel tous les √©tats sont dits ¬´ markoviens ¬ª et sur lequel des d√©cisions prises auront une r√©compense √† chaque action pos√©e. Que veut-on dire par √©tats ¬´ markoviens ¬ª ?

- On parle √©galement dans ce cas d&#39;√©tats qui vont respecter la _propri√©t√© de Markov_
- Cela veut dire que l&#39;√©tat actuel est ind√©pendant des actions et des √©tats pass√©s
  - Avec l&#39;√©tat actuel, le fait de conna√Ætre le pass√© n&#39;apporterait pas d&#39;informations suppl√©mentaires pour pr√©dire le futur

Cela permet de faire de l&#39;apprentissage supervis√© car on se trouve dans une situation o√π l&#39;on dispose de :

- Un ensemble fini d&#39;√©tats (Chaque valeur des 128 bytes)
- Un ensemble fini d&#39;actions (Les 4 actions possibles : NOOP, FIRE, LEFT, RIGHT)
- Une matrice de probabilit√© de transition d&#39;√©tat (inconnue au d√©part, mais le mod√®le va tenter de le construire)
- Une fonction de r√©compense (qui sera construite et mise en place par le mod√®le au fur et √† mesure de la d√©couverte des diff√©rents √©tats)
- Facteur de d√©valuation que l&#39;on pourra utiliser pour entrainer le mod√®le

Voici le processus de d√©cision de markov pr√©liminaire qui serait applicable au jeu ¬´ Breakout ¬ª :

![Breakout](images/PDM.png)

# Une premi√®re approche : l&#39;algorithme DQN

Pour d√©buter le travail de recherche et l&#39;entrainement d&#39;un mod√®le, j&#39;ai d&#39;abord choisi d&#39;explorer le mod√®le ¬´ Deep Q Network ¬ª. On parle ici d&#39;apprentissage par renforcement combin√© √† l&#39;apprentissage profond. Le concept est ing√©nieux et s&#39;appuie sur la notion de m√©moire de ¬´ replay ¬ª qui permet √† l&#39;algorithme d&#39;√©chantillonner les s√©quences pr√©sentes dans cette m√©moire et de calculer les cibles recherch√©es √† chaque action afin d&#39;optimiser l&#39;erreur moyenne carr√©e (MSE).

Concr√®tement, √† chaque temps ¬´ t ¬ª, le mod√®le :

- Choisira l&#39;action at √† poser en utilisant une politique epsilon-gloutonne, visant √† balancer l&#39;apprentissage et l&#39;exploration
- Enregistrera la transition (st, at, rt+1, st+1) dans sa m√©moire de replay
- Prendre un √©chantillon de transitions (s, a, r, s&#39;) √† partir de la m√©moire de replay
- Optimiser le MSE entre le r√©seau cible et le r√©seau de pr√©diction, apr√®s avoir calcul√© les cibles

Pour ce faire, j&#39;ai utilis√© un epsilon sur le concept de _decay_, tel que vu en cours :

- On commence avec un epsilon √† 1
  - Concr√®tement, on explore!
- Et √† chaque fois que l&#39;on franchit un nombre n d&#39;actions, on diminue progressivement et relativement epsilon (√† hauteur de 97.5% de sa valeur dans mon cas)
  - On limite le minimum √† une valeur donn√©e, 1% dans mon cas, afin de garder une part d&#39;exploration de moins en moins grande, mais quand m√™me pr√©sente

Cela veut dire qu&#39;√† chaque action, le mod√®le va soit :

- Explorer
  - C&#39;est-√†-dire prendre une action au hasard parmi les 4 disponibles (NOOP, FIRE, LEFT, RIGHT)
- Demander au mod√®le profond de pr√©dire la meilleure action, bas√© sur les apprentissages pr√©c√©dents

Une fois l&#39;action choisie par le mod√®le, l&#39;environnement retourne 3 informations importantes :

- Le nouvel √©tat
- Une r√©compense (0 ou 1)
- Un indicateur de fin de partie (done)

On prend alors ces informations et on les donne au mod√®le pour qu&#39;il apprenne √† chaque fois en lui disant, si je vulgarise :

- Voici l&#39;√©tat o√π j&#39;√©tais et l&#39;action que j&#39;ai pos√©e
- Voici l&#39;√©tat dans lequel je me trouve maintenant et la r√©compense obtenue
- Et au fait, le jeu m&#39;a dit que la partie √©tait termin√©e ou non

Le concept de DQN utilise √©galement 2 r√©seaux de neurones distincts afin de construire les r√©sultats obtenus :

- Le mod√®le lui-m√™me et le mod√®le cible
- √Ä chaque apprentissage, le mod√®le est actualis√© en fonction de l&#39;√©chantillonnage sur la m√©moire de replay et les pr√©dictions du mod√®le cible
- Le mod√®le cible est ensuite actualis√© en fonction du mod√®le suivant un facteur _tau_ pour relativiser les poids

## C&#39;est bien beau tout √ßa, mais concr√®tement, comment s&#39;est pass√© le tout me demanderez-vous?

Disons qu&#39;au d√©part, j&#39;avais vraiment l&#39;impression d&#39;avoir les yeux band√©s et de tenter de frapper une cible.

Au d√©but de tout, j&#39;ai parfois constat√© que l&#39;algorithme √©tait vraiment tr√®s long √† entrainer et que rien ou √† peu pr√®s ne se passait. En examinant les s√©quences de jeu, je me suis rendu compte qu&#39;il arrivait, sur certaines s√©quences, que le commande ¬´ FIRE ¬ª ne soit pas envoy√©e avant un tr√®s long moment, ce qui pouvait fausser toute la s√©quence pr√©dictive, car le jeu roulait longtemps et la barre se d√©pla√ßait de gauche √† droite, mais la balle n&#39;√©tait jamais lanc√©e! C&#39;√©tait facile √† identifier apr√®s coup, car on voyait la longueur des √©pisodes plafonner au maximum de 2500 instants et faire des scores de 0.

J&#39;ai fait diff√©rents essais avec un r√©seau de neurones de 2X150 couches. L&#39;algorithme apprend un peu, mais c&#39;est surtout al√©atoire :

![](images/001.png)

J&#39;ai fait de nombreux essais (tr√®s longs √† rouler d&#39;ailleurs) et j&#39;obtenais toujours plus ou moins des r√©sultats semblable (un score autour de 6 points dans le meilleur des cas).

J&#39;ai alors commenc√© √† regarder ce qui pouvait se passer et pourquoi l&#39;apprentissage √©tait complexe pour l&#39;agent. Un des points qui me semble difficile pour un agent comme celui-ci, est que la perte de vie ne retourne pas de r√©compense n√©gative. L&#39;agent ne sait donc pas que perdre une vie est ¬´ grave ¬ª et ne sait qu&#39;en bout de piste qu&#39;il a √©puis√© ses vies.

J&#39;ai donc regard√© comment je pourrais d√©tecter si une vie avait √©t√© perdue; j&#39;ai fait cela en comparant les bytes de la RAM entre chaque √©tat et en validant le nombre de vies restantes affich√©es √† l&#39;√©cran vs l&#39;√©tat de la m√©moire :

![](images/Bytes.png)

J&#39;ai donc identifi√© le _byte_ qui m&#39;int√©ressait et lorsque l&#39;√©tat suivant avait une valeur inf√©rieure pour ce byte √† l&#39;√©tat actuel, je pouvais assur√©ment d√©tecter une perte de vie. J&#39;ai donc introduit un concept de r√©compense n√©gative √† -100. (Suite √† une discussion avec Mikael √† ce propos, il m&#39;a fait remarquer que ce n&#39;√©tait pas une bonne id√©e, car on se trouvait √† tricher l&#39;environnement et ne plus respecter le concept d&#39;apprentissage par renforcement dans un tel cas; je comprends tout √† fait, mais je vais quand m√™me exposer ce que cela m&#39;a permis d&#39;atteindre comme score).

J&#39;ai aussi voulu mettre un concept de p√©nalit√© sur le temps qui s&#39;√©chappe, pour tester, √† raison de -0.01; j&#39;ai cependant laisser tomber ce concept suite √† la discussion avec Mikael, bien que j&#39;ai gard√© le code pour le faire (je passe d√©sormais 0 aux 2 valeurs par d√©faut).

![](images/002.png)

Avec cette strat√©gie en place, j&#39;ai obtenu des r√©sultats avec un score de 15 points :

![](images/003.png)

J&#39;ai ensuite voulu pousser plus loin en ¬´ simulant ¬ª que les √©tats de vies demeuraient stables, de m√™mes que le score, car dans les faits, ces 2 valeurs ne devraient pas avoir d&#39;incidence sur l&#39;action √† poser. En effet, qu&#39;il reste 2 ou 5 vies, la d√©cision de bouger √† droite ou √† gauche ne devrait pas √™tre diff√©rente. M√™me chose pour le score. J&#39;ai plus tard abandonn√© ce concept √©galement, pour ne pas alt√©rer l&#39;environnement et l&#39;agent.

√Ä cet effet, j&#39;ai √©galement essay√© de limiter √† une vie les √©pisodes, de fa√ßon √† obtenir le meilleur score √† une vie (ultimement, on veut maximiser le score √† chaque vie et non sur la partie, car ce faisant on va maximiser le score de la partie, du moins c&#39;√©tait mon hypoth√®se). J&#39;ai obtenu de bons r√©sultats, √† savoir jusqu&#39;√† 9 points par vie (donc th√©oriquement un genre de 45 points si j&#39;extrapole et je suis optimiste sur les 5 vies).

![](images/004.png)

J&#39;ai par la suite tent√© d&#39;ajuster les couches de neurones en mettant 2X128, toujours √† une seule vie. Dans ce cas, on voit que √ßa a √©t√© vraiment long avant que le mod√®le ne d√©couvre que l&#39;action FIRE initiait le jeu, car le score demeure sensiblement le m√™me :

![](images/005.png)

### Retour au mod√®le sans p√©nalit√© de vie

Je suis revenu √† un mod√®le o√π je n&#39;alt√©rais pas les r√©compenses ni les √©tats. J&#39;ai d√©cid√© d&#39;essayer avec 4 couches de 128 neurones.

![](images/006.png)

On voit qu&#39;√† un certain moment autour du 500e √©pisodes, le mod√®le a pris plus de temps avant de lancer la balle, mais n&#39;a pas am√©lior√© son score pour autant. J&#39;ai obtenu 11 comme maximum dans ce cas. Un peu d√©courageant quand on pense que les meilleurs algorithmes peuvent aller jusqu&#39;√† 430 environ.

J&#39;ai donc d√©cid√© de regarder du c√¥t√© du mod√®le _Actor-Critic_, par curiosit√©.

# Prise 2 : Mod√®le _Actor-Critic_

Un peu d√©confit de mes r√©sultats obtenus, je me suis pench√© sur le concept d&#39;_Actor-Critic_ tel que vu dans le cours. Une des raisons est le temps de traitement qui est beaucoup (_vraiment_ beaucoup) plus rapide. On parle de 4-6 heures pour 1000 √©pisodes en DQN vs une heure environ en AC.

Concr√®tement, ce type de mod√®le s&#39;appuie sur 2 composants :

- L&#39;acteur, qui repr√©sente la politique responsable du choix des actions
- Le critique, qui repr√©sente l&#39;√©valuation de la qualit√© des actions choisies par l&#39;acteur
  - C&#39;est le critique qui y propose des am√©liorations

Ce mod√®le apprend √† la fois la politique et la fonction de valeur au cours de ses exp√©riences. On parle d&#39;avantage lorsque le mod√®le fait la diff√©rence entre la fonction d&#39;action et celle de valeur, car cela repr√©sente la r√©compense de plus qu&#39;on obtiendrait sur une action donn√©e versus si on s&#39;en tenait √† la politique.

### Premier essai avec 2 couches de 32 neurones, _alpha = 0.001, beta = 0.005_

![](images/007.png)

- Meilleur score obtenu: 9.0
- Dur√©e moyenne: 289.4 actions
- Score moyen: 1.92 points

J&#39;ai diminu√© _gamma_ √† 0.95 et ensuite √† 0.8 au lieu de 0.99, par curiosit√©, mais j&#39;ai obtenu des r√©sultats tr√®s similaires.

### Changement des valeurs alpha et beta (learning rate) √† 0.005 et 0.001

Je voulais voir l&#39;incidence du changement de _learning rate_ sur l&#39;acteur (alpha) et le critique (beta), voici les r√©sultats obtenus (pas tr√®s diff√©rents) :

![](images/008.png)

- Meilleur score obtenu: 7.0
- Dur√©e moyenne: 302.3 actions
- Score moyen: 1.27 points

# Conclusion et r√©sultats

J&#39;avoue √™tre rest√© sur ma faim face aux diff√©rents r√©sultats obtenus, j&#39;aurais aim√© pouvoir tirer des conclusions claires et avoir une r√©ponse simple et claire sur la finalit√©. Malheureusement, le meilleur score obtenu dans tous mes essais est de 15 points, ce qui est tr√®s peu.

Cela dit, je suis bien heureux d&#39;avoir pu exp√©rimenter et tester diff√©rents algorithmes; l&#39;univers de l&#39;apprentissage par renforcement est vaste et je vais certainement le revisiter au cours des prochains mois. J&#39;aime beaucoup l&#39;aspect ludique mis de l&#39;avant par des librairies comme OpenAI Gym, cela rend l&#39;apprentissage plus int√©ressant et on peut presque dire que c&#39;est hypnotisant en mode interactif de voir l&#39;agent tenter de r√©soudre et de jouer correctement.

J&#39;ai beaucoup aim√© l&#39;exp√©rience, mais je vais devoir m&#39;y remettre √† t√™te repos√©e pour explorer vraiment chaque diff√©rent algorithme. Mention honorable d&#39;ailleurs √† l&#39;algorithme SARSA pour trouver les chemins dans les premiers exercices, j&#39;ai vraiment ador√© le concept et l&#39;application de celui-ci, bien que je n&#39;en ai pas fait usage dans ce travail, les possibilit√©s d&#39;√©tats me semblant beaucoup trop grands pour √™tre envisageable (256^128).

Mention sp√©ciale √† tous ceux qui ont trouv√© leur compte dans les formules math√©matiques avanc√©es, j&#39;avais h√¢te de voir le code pour mieux comprendre tout cela. üòä



# R√©f√©rences

- [https://gym.openai.com/envs/Breakout-ram-v0/](https://gym.openai.com/envs/Breakout-ram-v0/)
- [https://deepsense.ai/playing-atari-on-ram-with-deep-q-learning/](https://deepsense.ai/playing-atari-on-ram-with-deep-q-learning/)
- [http://cs229.stanford.edu/proj2016/report/BonillaZengZheng- AsynchronousDeepQLearningforBreakout-Report.pdf](http://cs229.stanford.edu/proj2016/report/BonillaZengZheng-%20AsynchronousDeepQLearningforBreakout-Report.pdf)
- [https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756](https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756)
- [https://keras.io/examples/rl/deep\_q\_network\_breakout/](https://keras.io/examples/rl/deep_q_network_breakout/)
- [https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)
- [https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c](https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c)